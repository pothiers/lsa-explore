{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOAO data reduction\n",
    "### WESmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) 2018 \n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_rows', 32, 'max_columns', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some important fields: \n",
    "important = ['DATE-OBS', 'DTCALDAT', 'DTTELESC', 'DTINSTRU',\n",
    "             'OBSTYPE','PROCTYPE','PRODTYPE','DTSITE', 'OBSERVAT', \n",
    "             'REFERENCE','FILESIZE','MD5SUM','DTACQNAM','DTPROPID',\n",
    "             'PI','RELEASE_DATE','RA','DEC','FOOTPRINT','FILTER',\n",
    "             'EXPOSURE','OBSMODE','SEEING','DEPTH','SURVEYID',\n",
    "             'COLLECTIONID','OBJECT','RADIUS / BOX', 'RADIUS/BOX']  # note RADIUS/BOX with and without spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of test NOAO json data\n",
    "BASE = '~/data/json-scrape/mtn'\n",
    "# the following will depend upon the organization of NOAO data\n",
    "DATE = ['{}'.format(x) for x in range(20170701, 20170726)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDF5 storage of dataframe metadata from \n",
    "# https://stackoverflow.com/questions/29129095/save-additional-attributes-in-pandas-dataframe/29130146#29130146\n",
    "# note: needed to 'pip install --upgrade tables' for HDFStore\n",
    "\n",
    "def h5store(filename, df, **kwargs):\n",
    "    store = pd.HDFStore(filename)\n",
    "    store.put('mydata', df)\n",
    "    store.get_storer('mydata').attrs.metadata = kwargs\n",
    "    store.close()\n",
    "\n",
    "def h5load(store):\n",
    "    data = store['mydata']\n",
    "    metadata = store.get_storer('mydata').attrs.metadata\n",
    "    return data, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile ProcessJSON.txt\n",
    "# to write this cell out for printing, uncomment the line above: \n",
    "# otherwise leave it commented, or this cell will not compile\n",
    "\n",
    "class ProcessJSON(object):\n",
    "\n",
    "    def __init__(self, datadir, savdir='../pydata-book/processed', file_hdr='local_file'):\n",
    "        self._datadir   = datadir\n",
    "        self._savdir    = savdir\n",
    "        self._file_hdr  = file_hdr\n",
    "        self._txtfmt    = 'DATE:{}, {} FILES'  # date, number of files that date\n",
    "        self._savfmt    = '{}/{}-processed.hdf5' # savdir, date\n",
    "        self._errmsg    = 'ERROR: Need to run .run() method first!'\n",
    "        self._error_group_col = \\\n",
    "              'ERROR: grouping column {} in file {} does not have a unique value'\n",
    "            \n",
    "        self._metadata        = {}\n",
    "        self._date            = None\n",
    "        self._important       = None  # no self-importance here!\n",
    "        self._processed       = None\n",
    "        self._group_col       = None\n",
    "        self._num             = None\n",
    "        self._num_to_read     = None\n",
    "        self._full_dataframe  = None\n",
    "        self._multi_group_cols = None\n",
    "        self._force_overwrite  = False\n",
    "        \n",
    "        os.makedirs(self._savdir, exist_ok=True)\n",
    "        \n",
    "    def _get_file_list(self):\n",
    "        file_list = []\n",
    "        for dirpath, dirs, files in os.walk(os.path.join(self._datadir, self._date)): \n",
    "            for filename in fnmatch.filter(files, '*.json'):\n",
    "                file_list.append(os.path.join(dirpath, filename))\n",
    "        self._num = min(len(file_list), self._num_to_read)\n",
    "        self._file_list = file_list\n",
    "               \n",
    "    def _process(self):\n",
    "        '''process group of json files , save dataframe to disk'''\n",
    "        \n",
    "        self._get_file_list()\n",
    "        print('processing ', self._txtfmt.format(self._date, self._num))\n",
    "        \n",
    "        # if important keys are provided, make a dummy starting dataframe with those keys\n",
    "        dd = [] if self._important == None else [pd.DataFrame(columns=self._important)]\n",
    "        \n",
    "        for k in range(self._num):\n",
    "            jj = pd.read_json(self._file_list[k])\n",
    "            \n",
    "            # verify the grouping-column value is unique and not missing\n",
    "            # in this file across the HDUs, otherwise assert an error; \n",
    "            # TODO: make this a try/except: save bad filenames and keep moving\n",
    "            assert jj[self._group_col].nunique() == 1, \\\n",
    "                self._error_group_col.format(self._group_col, self._file_list[k])\n",
    "            \n",
    "            # if existing and unique, broadcast the grouping-column value\n",
    "            # to the entire grouping column: this is required for proper grouping later;\n",
    "            # usually grouping column is 'DTINSTRU', the instrument name\n",
    "            jj[self._group_col] = jj[self._group_col].dropna().iloc[0]\n",
    "            \n",
        "            # add the file-name column to the dataframe: \n",
    "            # this is required for grouping HDUs by filename\n",
    "            jj[self._file_hdr] = self._file_list[k][47:]\n",
    "            \n",
    "            dd.append(jj)\n",
    "            \n",
    "        # if important keys are provided, cull the dataframe with those keys: \n",
    "        # do this AFTER concat with dummy frame with all the important keys, \n",
    "        # otherwise smaller frames may not have all of the keys\n",
    "        self._processed = pd.concat(dd) if self._important == None else \\\n",
    "                          pd.concat(dd)[self._important]\n",
    "            \n",
    "        self._metadata['num_files']   = self._num\n",
    "        self._metadata['date_record'] = self._date\n",
    "        h5store(self._savfile, self._processed, **self._metadata)\n",
    "        \n",
    "    def _get_data(self, date):\n",
    "        self._date = date\n",
    "        self._savfile = self._savfmt.format(self._savdir, self._date)\n",
    "        if (os.path.isfile(self._savfile) and not self._force_overwrite):\n",
    "            print('reading {} from disk'.format(self._savfile))\n",
    "            with pd.HDFStore(self._savfile) as store:\n",
    "                self._processed, self._metadata = h5load(store)\n",
    "        else:\n",
    "            self._process()\n",
    "                    \n",
    "    def run(self, date_range, group_col='DTINSTRU', important=None, \n",
    "            num_to_read=None, force_overwrite=False):  \n",
    "        '''\n",
    "        date_range:      list of date directories to read\n",
    "        group_col:       column name on which to group: usually 'DTINSTRU'\n",
    "        important:       list of columns to keep in processed dataframes\n",
    "        num_to_read:     number of files to read \n",
    "                         (default: read all files in each date directory)\n",
    "        force_overwrite: if processed dataframe exists on disk, \n",
    "                         overwrite if True (default: False)\n",
    "        '''\n",
    "        raw = []\n",
    "        self._num_to_read = np.iinfo(np.int32).max if num_to_read == None else \\\n",
    "                            num_to_read\n",
    "        self._group_col   = group_col\n",
    "        self._multi_group_cols  = [self._group_col, self._file_hdr]\n",
    "\n",
    "        # add file-header column for filename: \n",
    "        # important not to use append() method here: it breaks things\n",
    "        self._important = important + [self._file_hdr] \n",
    "        self._force_overwrite = force_overwrite\n",
    "        \n",
    "        # ensure date_range is a list if a scalar is input\n",
    "        date_range = date_range if isinstance(date_range, list) else [date_range]\n",
    "        \n",
    "        for k in date_range:\n",
    "            self._get_data(k)\n",
    "            raw.append(self._processed)\n",
    "        self._full_dataframe = pd.concat(raw)\n",
    "        \n",
    "    @property\n",
    "    def get_full_dataframe(self):\n",
    "        assert self._full_dataframe is not None, self._errmsg\n",
    "        return self._full_dataframe\n",
    "    \n",
    "    @property\n",
    "    def get_instr_vs_fields_unique_all_data(self):\n",
    "        # TODO: this needs to be generalized so user can define the top rows \n",
    "        #       for display: \n",
    "        #       this will be broken when the 'important' list changes\n",
    "        gg = self.get_full_dataframe.groupby(self._group_col).nunique().T\n",
    "        indx = list(gg.index)\n",
    "        # reorder rows to get similar rows at top for direct comparison\n",
    "        indx = [self._file_hdr,'DTACQNAM'] + indx[:12] + indx[13:-1]\n",
    "        return gg.loc[indx,:]\n",
    "    \n",
    "    @property\n",
    "    def get_HDU_uniqueness_per_file(self):\n",
    "        gg = self.get_full_dataframe.groupby(self._multi_group_cols).nunique()\n",
    "        # drop corrupted (by nunique()) grouping columns\n",
    "        gg = gg.drop(self._multi_group_cols, axis=1)\n",
    "        # reset index, drop unnecessary local_file column\n",
    "        gg.reset_index().drop(self._multi_group_cols[1], axis=1)  \n",
    "        return gg.groupby(self._multi_group_cols[0]).\\\n",
    "                  agg(['min','max','mean','std']).round(2).stack().T\n",
    "    \n",
    "    @property\n",
    "    def get_all_fields(self):\n",
    "        return self._important\n",
    "    \n",
    "    @property    \n",
    "    def get_HDU_stats(self):\n",
    "        gg = self.get_full_dataframe.groupby(self._multi_group_cols).size()\n",
    "        return gg.groupby(self._group_col).agg(['min','max','mean','std']).\\\n",
    "                                         rename_axis('HDU stats:', axis=1)\n",
    "    \n",
    "    def get_num_files_writing_fields(self, instr=True, percent=True):\n",
    "        '''\n",
    "        instr:   if True, list percentages (or raw numbers) of files per \n",
    "                 instrument that write each field, if False list total\n",
    "                 number of files (or percentages) over ALL instruments \n",
    "                 (default=True)\n",
    "        percent: if True, list percentages of files that write each field, \n",
    "                 if False, list raw numbers of files (default=True)\n",
    "        '''\n",
    "        zz = self.get_full_dataframe.groupby(self._multi_group_cols).nunique() > 0\n",
    "        if not instr:\n",
    "            gg = zz.sum()\n",
    "            return (gg/gg[self._file_hdr]*100).round(2) if percent else gg\n",
    "        else:\n",
    "            gg = zz.drop(['DTINSTRU'], axis=1).\\\n",
    "                 rename(columns={self._file_hdr:'COUNT'}).\\\n",
    "                 reset_index().drop(self._file_hdr, axis=1)\n",
    "            gg = gg.groupby('DTINSTRU').sum().T\n",
    "            return (gg/gg.loc['COUNT']*100).round(2) if percent else gg\n",
    "\n",
    "    def get_unique_values_of_field(self, field):\n",
    "        return list(self.get_full_dataframe[field].dropna().unique())\n",
    "    \n",
    "    def get_num_unique_values_by_keys(self, field1, field2):\n",
    "        gg = self.get_full_dataframe.groupby([field1, field2]).nunique()\n",
    "        return pd.DataFrame(gg.loc[:, self._file_hdr]).rename(columns=\\\n",
    "                                            {self._file_hdr:'TOTAL OCCURRENCES'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = ProcessJSON(BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing  DATE:20170701, 0 FILES\n",
      "processing  DATE:20170702, 0 FILES\n",
      "processing  DATE:20170703, 0 FILES\n",
      "processing  DATE:20170704, 0 FILES\n",
      "processing  DATE:20170705, 0 FILES\n",
      "processing  DATE:20170706, 0 FILES\n",
      "processing  DATE:20170707, 0 FILES\n",
      "processing  DATE:20170708, 0 FILES\n",
      "processing  DATE:20170709, 0 FILES\n",
      "processing  DATE:20170710, 0 FILES\n",
      "processing  DATE:20170711, 0 FILES\n",
      "processing  DATE:20170712, 0 FILES\n",
      "processing  DATE:20170713, 0 FILES\n",
      "processing  DATE:20170714, 0 FILES\n",
      "processing  DATE:20170715, 0 FILES\n",
      "processing  DATE:20170716, 0 FILES\n",
      "processing  DATE:20170717, 0 FILES\n",
      "processing  DATE:20170718, 0 FILES\n",
      "processing  DATE:20170719, 0 FILES\n",
      "processing  DATE:20170720, 0 FILES\n",
      "processing  DATE:20170721, 0 FILES\n",
      "processing  DATE:20170722, 0 FILES\n",
      "processing  DATE:20170723, 0 FILES\n",
      "processing  DATE:20170724, 0 FILES\n",
      "processing  DATE:20170725, 0 FILES\n"
     ]
    }
   ],
   "source": [
    "dates = DATE\n",
    "num = None #100  # 'None' to get all files\n",
    "force_overwrite = False\n",
    "proc.run(dates, important=important, group_col='DTINSTRU', num_to_read=num, force_overwrite=force_overwrite) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc.get_unique_values_of_field('DTINSTRU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = proc.get_full_dataframe.copy()  # make copy to experiment: without copying, it is a VIEW (ie, a pointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = proc.get_instr_vs_fields_unique_all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = proc.get_HDU_uniqueness_per_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = proc.get_all_fields # list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee = proc.get_HDU_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = proc.get_unique_values_of_field('OBSTYPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg1 = proc.get_num_unique_values_by_keys('DTINSTRU', 'OBSTYPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg2 = proc.get_num_unique_values_by_keys('DTINSTRU', 'FILTER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg3 = proc.get_num_unique_values_by_keys('DTTELESC','DTINSTRU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg4 = proc.get_num_unique_values_by_keys('DTINSTRU', 'DTCALDAT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh1 = proc.get_num_files_writing_fields(instr=True, percent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh2 = proc.get_num_files_writing_fields(instr=True, percent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh3 = proc.get_num_files_writing_fields(instr=False, percent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh4 = proc.get_num_files_writing_fields(instr=False, percent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE-OBS</th>\n",
       "      <th>DTCALDAT</th>\n",
       "      <th>DTTELESC</th>\n",
       "      <th>DTINSTRU</th>\n",
       "      <th>OBSTYPE</th>\n",
       "      <th>PROCTYPE</th>\n",
       "      <th>PRODTYPE</th>\n",
       "      <th>DTSITE</th>\n",
       "      <th>OBSERVAT</th>\n",
       "      <th>REFERENCE</th>\n",
       "      <th>FILESIZE</th>\n",
       "      <th>MD5SUM</th>\n",
       "      <th>DTACQNAM</th>\n",
       "      <th>DTPROPID</th>\n",
       "      <th>PI</th>\n",
       "      <th>RELEASE_DATE</th>\n",
       "      <th>RA</th>\n",
       "      <th>DEC</th>\n",
       "      <th>FOOTPRINT</th>\n",
       "      <th>FILTER</th>\n",
       "      <th>EXPOSURE</th>\n",
       "      <th>OBSMODE</th>\n",
       "      <th>SEEING</th>\n",
       "      <th>DEPTH</th>\n",
       "      <th>SURVEYID</th>\n",
       "      <th>COLLECTIONID</th>\n",
       "      <th>OBJECT</th>\n",
       "      <th>RADIUS / BOX</th>\n",
       "      <th>RADIUS/BOX</th>\n",
       "      <th>local_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [DATE-OBS, DTCALDAT, DTTELESC, DTINSTRU, OBSTYPE, PROCTYPE, PRODTYPE, DTSITE, OBSERVAT, REFERENCE, FILESIZE, MD5SUM, DTACQNAM, DTPROPID, PI, RELEASE_DATE, RA, DEC, FOOTPRINT, FILTER, EXPOSURE, OBSMODE, SEEING, DEPTH, SURVEYID, COLLECTIONID, OBJECT, RADIUS / BOX, RADIUS/BOX, local_file]\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa # too big for html: 389000 rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make optional csv, html output a method in ProcessJSON\n",
    "bb.to_html('html/get_instr_vs_fields_unique_all_data.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb.to_csv('csv/get_instr_vs_fields_unique_all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.to_html('html/get_HDU_uniqueness_per_file.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.to_csv('csv/get_HDU_uniqueness_per_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DATE-OBS',\n",
       " 'DTCALDAT',\n",
       " 'DTTELESC',\n",
       " 'DTINSTRU',\n",
       " 'OBSTYPE',\n",
       " 'PROCTYPE',\n",
       " 'PRODTYPE',\n",
       " 'DTSITE',\n",
       " 'OBSERVAT',\n",
       " 'REFERENCE',\n",
       " 'FILESIZE',\n",
       " 'MD5SUM',\n",
       " 'DTACQNAM',\n",
       " 'DTPROPID',\n",
       " 'PI',\n",
       " 'RELEASE_DATE',\n",
       " 'RA',\n",
       " 'DEC',\n",
       " 'FOOTPRINT',\n",
       " 'FILTER',\n",
       " 'EXPOSURE',\n",
       " 'OBSMODE',\n",
       " 'SEEING',\n",
       " 'DEPTH',\n",
       " 'SURVEYID',\n",
       " 'COLLECTIONID',\n",
       " 'OBJECT',\n",
       " 'RADIUS / BOX',\n",
       " 'RADIUS/BOX',\n",
       " 'local_file']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd  # list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.to_html('html/get_HDU_stats.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.to_csv('csv/get_HDU_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg1.to_html('html/get_num_unique_values_by_keys_DTINSTRU_OBSTYPE.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg1.to_csv('csv/get_num_unique_values_by_keys_DTINSTRU_OBSTYPE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gg2.to_html('html/get_num_unique_values_by_keys_DTINSTRU_FILTER.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gg2.to_csv('csv/get_num_unique_values_by_keys_DTINSTRU_FILTER.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg3.to_html('html/get_num_unique_values_by_keys_DTTELESC_DTINSTRU.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg3.to_csv('csv/get_num_unique_values_by_keys_DTTELESC_DTINSTRU.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg4.to_html('html/get_num_unique_values_by_keys_DTINSTRU_DTCALDAT.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg4.to_csv('csv/get_num_unique_values_by_keys_DTINSTRU_DTCALDAT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh1.to_html('html/get_num_files_writing_fields(instr=True, percent=True).html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh2.to_html('html/get_num_files_writing_fields(instr=True, percent=False).html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh3.to_frame().to_html('html/get_num_files_writing_fields(instr=False, percent=True).html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh4.to_frame().to_html('html/get_num_files_writing_fields(instr=False, percent=False).html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
